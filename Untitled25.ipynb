{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KlimLe/test/blob/main/Untitled25.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas numpy spacy yfinance scikit-learn transformers tensorflow nltk ta textblob"
      ],
      "metadata": {
        "id": "QJdh1fV8uMqq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LiAI-s01tKrC"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import spacy\n",
        "import yfinance as yf\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split, TimeSeriesSplit\n",
        "from sklearn.impute import KNNImputer\n",
        "from transformers import RobertaTokenizer, TFRobertaModel\n",
        "import tensorflow as tf\n",
        "import nltk\n",
        "from ta import add_all_ta_features\n",
        "from textblob import TextBlob\n",
        "import re\n",
        "\n",
        "# Load new financial news dataset\n",
        "news_data = pd.read_csv('/content/first_200_rows_dataset.csv')  # Replace with your dataset path\n",
        "news_data['Date'] = pd.to_datetime(news_data['Date'])\n",
        "news_data.rename(columns={'News Article': 'News_Article', 'Date': 'Date'}, inplace=True)\n",
        "\n",
        "# Download NLTK data\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Initialize Spacy model and NLTK components\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "stop_words = set(nltk.corpus.stopwords.words('english'))\n",
        "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
        "\n",
        "# List of companies to focus on\n",
        "companies_to_focus = {\n",
        "    'AMZN': 'Amazon',\n",
        "    'GOOGL': 'Google',\n",
        "    'AAPL': 'Apple'\n",
        "}\n",
        "\n",
        "# Function to preprocess text\n",
        "def preprocess_text(text):\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text, re.I | re.A)\n",
        "    text = text.lower()\n",
        "    text = text.strip()\n",
        "    tokens = text.split()\n",
        "    tokens = [word for word in tokens if word not in stop_words]\n",
        "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
        "    processed_text = ' '.join(tokens)\n",
        "    return processed_text\n",
        "\n",
        "# Preprocess news articles\n",
        "news_data['Processed_Article'] = news_data['News_Article'].apply(preprocess_text)\n",
        "\n",
        "# Perform Sentiment Analysis\n",
        "def get_sentiment(text):\n",
        "    return TextBlob(text).sentiment.polarity\n",
        "\n",
        "news_data[\"Sentiment\"] = news_data[\"Processed_Article\"].apply(get_sentiment)\n",
        "\n",
        "# Initialize BERT tokenizer and model (You can also use RoBERTa or other advanced models)\n",
        "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
        "bert_model = TFRobertaModel.from_pretrained('roberta-base')\n",
        "\n",
        "def get_bert_embeddings(texts, tokenizer, model):\n",
        "    inputs = tokenizer(texts, return_tensors=\"tf\", padding=True, truncation=True, max_length=128)\n",
        "    outputs = model(inputs)\n",
        "    return outputs.last_hidden_state[:, 0, :].numpy()  # Use the [CLS] token's embedding\n",
        "\n",
        "# Calculate BERT embeddings for all news\n",
        "news_data[\"BERT_Embedding\"] = news_data[\"Processed_Article\"].apply(lambda x: get_bert_embeddings([x], tokenizer, bert_model)[0])\n",
        "\n",
        "# Function to fetch stock prices and fundamental data for each company\n",
        "def fetch_stock_prices(ticker, start_date, end_date):\n",
        "    try:\n",
        "        stock_data = yf.download(ticker, start=start_date, end=end_date)\n",
        "        if stock_data.shape[0] > 14:  # Ensure there are at least 15 rows of data\n",
        "            stock_data = add_all_ta_features(stock_data, open=\"Open\", high=\"High\", low=\"Low\", close=\"Close\", volume=\"Volume\")\n",
        "            # Handle missing technical indicators\n",
        "            imputer = KNNImputer(n_neighbors=5)\n",
        "            stock_data.iloc[:, :] = imputer.fit_transform(stock_data)\n",
        "        else:\n",
        "            print(f\"Not enough data for {ticker}\")\n",
        "            return pd.DataFrame()\n",
        "\n",
        "        # Filter out rows with missing stock prices\n",
        "        stock_data.dropna(subset=['Close'], inplace=True)\n",
        "\n",
        "        # Reset index to get the date column back after filtering\n",
        "        stock_data.reset_index(inplace=True)\n",
        "\n",
        "        return stock_data\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching data for {ticker}: {e}\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "def fetch_fundamental_data(ticker):\n",
        "    stock = yf.Ticker(ticker)\n",
        "    fundamentals = stock.info\n",
        "    return {\n",
        "        \"PE_Ratio\": fundamentals.get(\"trailingPE\", np.nan),\n",
        "        \"EPS\": fundamentals.get(\"trailingEps\", np.nan),\n",
        "        \"Revenue\": fundamentals.get(\"totalRevenue\", np.nan),\n",
        "        \"Market_Cap\": fundamentals.get(\"marketCap\", np.nan)\n",
        "    }\n",
        "\n",
        "# Correct date format and optionally extend the date range\n",
        "from_date = \"2021-01-01\"\n",
        "to_date = \"2021-12-31\"  # Extended date range\n",
        "\n",
        "# Define look-back window\n",
        "look_back = 5\n",
        "\n",
        "# Function to prepare data for each company\n",
        "def prepare_company_data(ticker, company, from_date, to_date):\n",
        "    print(f\"Fetching data for {company} ({ticker})\")\n",
        "    stock_data = fetch_stock_prices(ticker, from_date, to_date)\n",
        "    if stock_data.empty:\n",
        "        print(f\"No stock data found for {company} ({ticker})\")\n",
        "        return None\n",
        "    fundamental_data = fetch_fundamental_data(ticker)\n",
        "\n",
        "    # Filter news for the company or its ticker symbol\n",
        "    company_news = news_data[news_data['News_Article'].str.contains(company, case=False) | news_data['News_Article'].str.contains(ticker, case=False)]\n",
        "\n",
        "    # Aggregate all news by day\n",
        "    all_news_agg = news_data.groupby('Date').agg({\n",
        "        'BERT_Embedding': lambda x: np.mean(np.vstack(x), axis=0),\n",
        "        'Sentiment': 'mean'\n",
        "    }).reset_index()\n",
        "\n",
        "    # Handle missing dates for all news\n",
        "    all_dates = pd.date_range(start=from_date, end=to_date, freq='D')\n",
        "    all_news_agg = all_news_agg.set_index('Date').reindex(all_dates).reset_index()\n",
        "    all_news_agg.rename(columns={'index': 'Date'}, inplace=True)\n",
        "\n",
        "    # Insert neutral values for missing dates\n",
        "    all_news_agg['BERT_Embedding'] = all_news_agg['BERT_Embedding'].apply(lambda x: x if isinstance(x, np.ndarray) else np.zeros(bert_model.config.hidden_size))\n",
        "    all_news_agg['Sentiment'] = all_news_agg['Sentiment'].fillna(0.0)\n",
        "\n",
        "    # Aggregate company-specific news by day\n",
        "    if not company_news.empty:\n",
        "        company_news_agg = company_news.groupby('Date').agg({\n",
        "            'BERT_Embedding': lambda x: np.mean(np.vstack(x), axis=0),\n",
        "            'Sentiment': 'mean'\n",
        "        }).reset_index()\n",
        "\n",
        "        # Handle missing dates for company-specific news\n",
        "        company_news_agg = company_news_agg.set_index('Date').reindex(all_dates).reset_index()\n",
        "        company_news_agg.rename(columns={'index': 'Date'}, inplace=True)\n",
        "\n",
        "        # Insert neutral values for missing dates\n",
        "        company_news_agg['BERT_Embedding'] = company_news_agg['BERT_Embedding'].apply(lambda x: x if isinstance(x, np.ndarray) else np.zeros(bert_model.config.hidden_size))\n",
        "        company_news_agg['Sentiment'] = company_news_agg['Sentiment'].fillna(0.0)\n",
        "    else:\n",
        "        # Create empty DataFrame with the same structure\n",
        "        company_news_agg = pd.DataFrame({\n",
        "            'Date': all_dates,\n",
        "            'BERT_Embedding': [np.zeros(bert_model.config.hidden_size)] * len(all_dates),\n",
        "            'Sentiment': [0.0] * len(all_dates)\n",
        "        })\n",
        "\n",
        "    # Ensure the columns have correct suffixes\n",
        "    company_news_agg.rename(columns={'BERT_Embedding': 'BERT_Embedding_company', 'Sentiment': 'Sentiment_company'}, inplace=True)\n",
        "    all_news_agg.rename(columns={'BERT_Embedding': 'BERT_Embedding_all', 'Sentiment': 'Sentiment_all'}, inplace=True)\n",
        "\n",
        "    # Merge stock data with aggregated news data\n",
        "    data = pd.merge(stock_data, company_news_agg, on=\"Date\", how=\"left\")\n",
        "    data = pd.merge(data, all_news_agg, on=\"Date\", how=\"left\")\n",
        "\n",
        "    # Add fundamental data (same value for all rows as an example)\n",
        "    for key, value in fundamental_data.items():\n",
        "        data[key] = value\n",
        "\n",
        "    data[\"Company_Name\"] = company\n",
        "\n",
        "    # Add future price column\n",
        "    data[\"Future_Price\"] = data[\"Close\"].shift(-1)  # Shift price for prediction\n",
        "\n",
        "    # Drop rows where the future price is missing (typically the last row)\n",
        "    data.dropna(subset=['Future_Price'], inplace=True)\n",
        "\n",
        "    # Impute missing values in technical indicators and fundamentals\n",
        "    technical_indicator_columns = data.filter(like='ta_').columns\n",
        "    for column in technical_indicator_columns:\n",
        "        data[column].fillna(method='ffill', inplace=True)\n",
        "        data[column].fillna(method='bfill', inplace=True)\n",
        "\n",
        "    fundamental_columns = [\"PE_Ratio\", \"EPS\", \"Revenue\", \"Market_Cap\"]\n",
        "    for column in fundamental_columns:\n",
        "        data[column].fillna(method='ffill', inplace=True)\n",
        "        data[column].fillna(method='bfill', inplace=True)\n",
        "\n",
        "    return data\n",
        "\n",
        "# Prepare data for each company\n",
        "all_company_data = {ticker: prepare_company_data(ticker, company, from_date, to_date) for ticker, company in companies_to_focus.items()}\n",
        "\n",
        "# Check for and remove any None entries\n",
        "all_company_data = {ticker: data for ticker, data in all_company_data.items() if data is not None}\n",
        "\n",
        "if not all_company_data:\n",
        "    raise ValueError(\"No data available for any company in the specified date range.\")\n",
        "\n",
        "# Create sequences for each company\n",
        "def create_sequences(data, look_back):\n",
        "    sequences = []\n",
        "    targets = []\n",
        "    for i in range(len(data) - look_back):\n",
        "        sequence = {\n",
        "            \"news_embeddings_company\": np.stack(data[\"BERT_Embedding_company\"].values[i:i+look_back]),\n",
        "            \"news_embeddings_all\": np.stack(data[\"BERT_Embedding_all\"].values[i:i+look_back]),\n",
        "            \"price\": data[\"Close\"].values[i:i+look_back].reshape(-1, 1),\n",
        "            \"sentiment_company\": data[\"Sentiment_company\"].values[i:i+look_back].reshape(-1, 1),\n",
        "            \"sentiment_all\": data[\"Sentiment_all\"].values[i:i+look_back].reshape(-1, 1),\n",
        "            \"technical_indicators\": data.filter(like='ta_').values[i:i+look_back],\n",
        "            \"fundamentals\": data[[\"PE_Ratio\", \"EPS\", \"Revenue\", \"Market_Cap\"]].values[i:i+look_back]\n",
        "        }\n",
        "        sequences.append(sequence)\n",
        "        targets.append(data[\"Future_Price\"].values[i + look_back])  # Correctly assign the future price as target\n",
        "    return sequences, np.array(targets)\n",
        "\n",
        "company_sequences = {ticker: create_sequences(data, look_back) for ticker, data in all_company_data.items()}\n",
        "\n",
        "# Ensure consistency of lengths\n",
        "min_length = min(len(sequences) for sequences, _ in company_sequences.values())\n",
        "company_sequences = {ticker: (sequences[:min_length], targets[:min_length]) for ticker, (sequences, targets) in company_sequences.items()}\n",
        "\n",
        "# Convert sequences to arrays for model input\n",
        "def convert_sequences(sequences):\n",
        "    news_embeddings_company = np.array([seq[\"news_embeddings_company\"] for seq in sequences])\n",
        "    news_embeddings_all = np.array([seq[\"news_embeddings_all\"] for seq in sequences])\n",
        "    price = np.array([seq[\"price\"] for seq in sequences])\n",
        "    sentiment_company = np.array([seq[\"sentiment_company\"] for seq in sequences])\n",
        "    sentiment_all = np.array([seq[\"sentiment_all\"] for seq in sequences])\n",
        "    technical_indicators = np.array([seq[\"technical_indicators\"] for seq in sequences])\n",
        "    fundamentals = np.array([seq[\"fundamentals\"] for seq in sequences])\n",
        "    return news_embeddings_company, news_embeddings_all, price, sentiment_company, sentiment_all, technical_indicators, fundamentals\n",
        "\n",
        "company_features = {ticker: (convert_sequences(sequences), targets) for ticker, (sequences, targets) in company_sequences.items()}\n",
        "\n",
        "# Validate lengths of the features\n",
        "for key, (value, targets) in company_features.items():\n",
        "    print(f\"{key} lengths: {[len(x) for x in value]}, targets length: {len(targets)}\")\n",
        "# Combine all features into a single array\n",
        "def combine_features(features):\n",
        "    combined = np.concatenate([features[0],\n",
        "                               features[1],\n",
        "                               features[2],\n",
        "                               features[3],\n",
        "                               features[4],\n",
        "                               features[5],\n",
        "                               features[6]], axis=-1)\n",
        "    return combined\n",
        "\n",
        "combined_features = {ticker: combine_features(features) for ticker, (features, _) in company_features.items()}\n",
        "combined_features_array = np.concatenate(list(combined_features.values()), axis=0)\n",
        "\n",
        "# Concatenate all targets into a single array along the correct axis\n",
        "targets_array = np.concatenate([targets.reshape(-1, 1) for _, targets in company_features.values()], axis=0)\n",
        "\n",
        "# Ensure the shape of targets matches the expected dimensions\n",
        "targets_array = targets_array.reshape(-1, len(companies_to_focus))\n",
        "\n",
        "# Convert targets to a DataFrame for multi-output regression\n",
        "targets_df = pd.DataFrame(targets_array, columns=companies_to_focus.keys())\n",
        "\n",
        "# Scale features\n",
        "scaler = StandardScaler()\n",
        "combined_features_array_scaled = scaler.fit_transform(combined_features_array.reshape(-1, combined_features_array.shape[-1]))\n",
        "combined_features_array_scaled = combined_features_array_scaled.reshape(combined_features_array.shape)\n",
        "\n",
        "# Scale the targets (future prices) individually for each company\n",
        "target_scalers = {ticker: StandardScaler() for ticker in companies_to_focus.keys()}\n",
        "targets_array_scaled = np.zeros_like(targets_array)\n",
        "\n",
        "for i, ticker in enumerate(companies_to_focus.keys()):\n",
        "    targets_array_scaled[:, i] = target_scalers[ticker].fit_transform(targets_array[:, i].reshape(-1, 1)).flatten()\n",
        "\n",
        "# Convert targets to a DataFrame for multi-output regression\n",
        "targets_df_scaled = pd.DataFrame(targets_array_scaled, columns=companies_to_focus.keys())\n",
        "\n",
        "# Ensure the number of samples is the same\n",
        "if combined_features_array.shape[0] != targets_df_scaled.shape[0]:\n",
        "    min_samples = min(combined_features_array.shape[0], targets_df_scaled.shape[0])\n",
        "    combined_features_array = combined_features_array[:min_samples]\n",
        "    targets_df_scaled = targets_df_scaled.iloc[:min_samples]\n",
        "\n",
        "# Prepare your data\n",
        "tscv = TimeSeriesSplit(n_splits=5)\n",
        "for train_index, val_index in tscv.split(combined_features_array):\n",
        "    X_train, X_val = combined_features_array[train_index], combined_features_array[val_index]\n",
        "    y_train, y_val = targets_df_scaled.values[train_index], targets_df_scaled.values[val_index]\n",
        "\n",
        "# Define the model\n",
        "def build_model(look_back, combined_dim, num_companies, num_heads=12, ff_dim=128, dropout_rate=0.5):\n",
        "    combined_input = tf.keras.layers.Input(shape=(look_back, combined_dim), name='combined_input')\n",
        "\n",
        "    # Register the custom layer for deserialization\n",
        "    @tf.keras.utils.register_keras_serializable()\n",
        "    # Transformer block\n",
        "    class TransformerBlock(tf.keras.layers.Layer):\n",
        "        def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n",
        "            super(TransformerBlock, self).__init__()\n",
        "            self.att = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
        "            self.ffn = tf.keras.Sequential([\n",
        "                tf.keras.layers.Dense(ff_dim, activation=\"relu\"),\n",
        "                tf.keras.layers.Dense(embed_dim),\n",
        "            ])\n",
        "            self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "            self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "            self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "            self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "        def call(self, inputs, training):\n",
        "            attn_output = self.att(inputs, inputs)\n",
        "            attn_output = self.dropout1(attn_output, training=training)\n",
        "            out1 = self.layernorm1(inputs + attn_output)\n",
        "            ffn_output = self.ffn(out1)\n",
        "            ffn_output = self.dropout2(ffn_output, training=training)\n",
        "            return self.layernorm2(out1 + ffn_output)\n",
        "\n",
        "    transformer_block = TransformerBlock(combined_dim, num_heads, ff_dim, rate=dropout_rate)\n",
        "    x = transformer_block(combined_input)\n",
        "\n",
        "    # Global average pooling\n",
        "    x = tf.keras.layers.GlobalAveragePooling1D()(x)\n",
        "\n",
        "    # Dense layer with Batch Normalization and Dropout\n",
        "    x = tf.keras.layers.Dense(64, activation=\"relu\")(x)\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "    x = tf.keras.layers.Dropout(dropout_rate)(x)\n",
        "\n",
        "    # Output layers for each company\n",
        "    outputs = {ticker: tf.keras.layers.Dense(1, activation='linear', name=f'output_{ticker}')(x) for ticker in companies_to_focus.keys()}\n",
        "\n",
        "    # Create model\n",
        "    model = tf.keras.models.Model(inputs=combined_input, outputs=outputs)\n",
        "\n",
        "    # Compile model with a dictionary of losses\n",
        "    losses = {ticker: 'mse' for ticker in companies_to_focus.keys()}\n",
        "    model.compile(loss=losses, optimizer=tf.keras.optimizers.Adam())\n",
        "\n",
        "    return model\n",
        "\n",
        "look_back = 5  # Define the look_back as per your data\n",
        "combined_dim = combined_features_array.shape[-1]  # Combined dimension\n",
        "\n",
        "model = build_model(look_back, combined_dim, len(companies_to_focus), 12, 128, 0.5)\n",
        "\n",
        "# Define the number of epochs\n",
        "epochs = 50\n",
        "\n",
        "# Set batch size\n",
        "batch_size = 32\n",
        "\n",
        "# Early stopping callback\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "\n",
        "# Train the model\n",
        "model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=epochs, batch_size=batch_size, callbacks=[early_stopping])\n",
        "\n",
        "# Make predictions on validation data\n",
        "predicted_prices_scaled = model.predict(X_val)\n",
        "\n",
        "# Inverse transform the predictions to get the original scale\n",
        "predicted_prices = {ticker: target_scalers[ticker].inverse_transform(predictions) for ticker, predictions in predicted_prices_scaled.items()}\n",
        "\n",
        "# Display the predicted prices in the original scale\n",
        "print(predicted_prices)\n",
        "\n",
        "# Save the retrained model\n",
        "model.save('trained_model.h5')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "!pip install streamlit pandas numpy yfinance tensorflow transformers plotly textblob scikit-learn gdown\n",
        "\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import yfinance as yf\n",
        "import tensorflow as tf\n",
        "from datetime import datetime, timedelta\n",
        "from transformers import RobertaTokenizer, TFRobertaModel\n",
        "import plotly.graph_objs as go\n",
        "from textblob import TextBlob\n",
        "import re\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import gdown\n",
        "\n",
        "# Download the model from Google Drive\n",
        "url = 'https://drive.google.com/uc?id=1TbZ9NSqYTTT3dndNBsGw11GHrKZ2W4zM'\n",
        "output = 'trained_model.h5'\n",
        "gdown.download(url, output, quiet=False)\n",
        "\n",
        "# Download the dataset from Google Drive\n",
        "dataset_url = 'https://drive.google.com/uc?id=1znZlJdW7WrJGC4DDtDFQ0aPz4k4f44bu'\n",
        "dataset_output = 'modified_first_200_rows_dataset.csv'\n",
        "gdown.download(dataset_url, dataset_output, quiet=False)\n",
        "\n",
        "# Define the company tickers and names\n",
        "companies_to_focus = {\n",
        "    'AMZN': 'Amazon',\n",
        "    'GOOGL': 'Google',\n",
        "    'AAPL': 'Apple'\n",
        "}\n",
        "\n",
        "# Initialize tokenizer and BERT model\n",
        "tokenizer = RobertaTokenizer.from_pretrained('roberta-base')\n",
        "bert_model = TFRobertaModel.from_pretrained('roberta-base')\n",
        "\n",
        "# Define lookback window\n",
        "look_back = 5\n",
        "\n",
        "# Register the custom layer for deserialization\n",
        "@tf.keras.utils.register_keras_serializable()\n",
        "class TransformerBlock(tf.keras.layers.Layer):\n",
        "    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1, **kwargs):\n",
        "        super(TransformerBlock, self).__init__(**kwargs)\n",
        "        self.att = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n",
        "        self.ffn = tf.keras.Sequential([\n",
        "            tf.keras.layers.Dense(ff_dim, activation=\"relu\"),\n",
        "            tf.keras.layers.Dense(embed_dim),\n",
        "        ])\n",
        "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
        "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
        "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
        "\n",
        "    def call(self, inputs, training):\n",
        "        attn_output = self.att(inputs, inputs)\n",
        "        attn_output = self.dropout1(attn_output, training=training)\n",
        "        out1 = self.layernorm1(inputs + attn_output)\n",
        "        ffn_output = self.ffn(out1)\n",
        "        ffn_output = self.dropout2(ffn_output, training=training)\n",
        "        return self.layernorm2(out1 + ffn_output)\n",
        "\n",
        "# Load the trained model with the custom layer\n",
        "custom_objects = {'TransformerBlock': TransformerBlock}\n",
        "model = tf.keras.models.load_model('trained_model.h5', custom_objects=custom_objects)\n",
        "\n",
        "# Function to preprocess text for BERT embeddings\n",
        "def preprocess_text(text):\n",
        "    text = re.sub(r'[^a-zA-Z\\s]', '', text, re.I | re.A)\n",
        "    text = text.lower().strip()\n",
        "    tokens = text.split()\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "# Function to get BERT embeddings\n",
        "def get_bert_embeddings(texts, tokenizer, model):\n",
        "    inputs = tokenizer(texts, return_tensors=\"tf\", padding=True, truncation=True, max_length=128)\n",
        "    outputs = model(inputs)\n",
        "    return outputs.last_hidden_state[:, 0, :].numpy()  # Use the [CLS] token's embedding\n",
        "\n",
        "# Function to predict future prices\n",
        "def predict_prices(news_headlines, look_back_window, bert_dim, combined_dim, scaler, target_scalers):\n",
        "    processed_articles = [preprocess_text(article) for article in news_headlines]\n",
        "    bert_embeddings = [get_bert_embeddings([article], tokenizer, bert_model)[0] for article in processed_articles]\n",
        "\n",
        "    # Ensure the embeddings have the correct shape\n",
        "    bert_embeddings = bert_embeddings[-look_back_window:]\n",
        "    if len(bert_embeddings) < look_back_window:\n",
        "        # Pad the embeddings if there are not enough look-back days\n",
        "        padding = [np.zeros((bert_dim,)) for _ in range(look_back_window - len(bert_embeddings))]\n",
        "        bert_embeddings = padding + bert_embeddings\n",
        "\n",
        "    if combined_dim > bert_dim:\n",
        "        # Combine with dummy data to match the expected combined dimension\n",
        "        dummy_data = np.zeros((look_back_window, combined_dim - bert_dim))\n",
        "        combined_features = np.concatenate([bert_embeddings, dummy_data], axis=-1)\n",
        "    else:\n",
        "        combined_features = np.array(bert_embeddings)\n",
        "\n",
        "    # Reshape for model input\n",
        "    combined_features = np.array(combined_features).reshape(1, look_back_window, -1)\n",
        "\n",
        "    # Scale the combined features\n",
        "    combined_features_scaled = scaler.transform(combined_features.reshape(-1, combined_features.shape[-1]))\n",
        "    combined_features_scaled = combined_features_scaled.reshape(combined_features.shape)\n",
        "\n",
        "    # Predict using the loaded model\n",
        "    predictions_scaled = model.predict(combined_features_scaled)\n",
        "\n",
        "    # Inverse transform the predictions to get the original scale\n",
        "    predictions = {ticker: target_scalers[ticker].inverse_transform(predictions_scaled[ticker]) for ticker in companies_to_focus.keys()}\n",
        "    return predictions\n",
        "\n",
        "# Function to perform sentiment analysis\n",
        "def get_sentiment(text):\n",
        "    return TextBlob(text).sentiment.polarity\n",
        "\n",
        "# Function to fetch fundamental data for a company\n",
        "def fetch_fundamental_data(ticker):\n",
        "    stock = yf.Ticker(ticker)\n",
        "    fundamentals = stock.info\n",
        "    return {\n",
        "        \"PE_Ratio\": fundamentals.get(\"trailingPE\", np.nan),\n",
        "        \"EPS\": fundamentals.get(\"trailingEps\", np.nan),\n",
        "        \"Revenue\": fundamentals.get(\"totalRevenue\", np.nan),\n",
        "        \"Market_Cap\": fundamentals.get(\"marketCap\", np.nan)\n",
        "    }\n",
        "\n",
        "# Load the dataset\n",
        "news_data = pd.read_csv('modified_first_200_rows_dataset.csv')\n",
        "news_data['Date'] = pd.to_datetime(news_data['Date'])\n",
        "news_data['Processed_Article'] = news_data['News Article'].apply(preprocess_text)\n",
        "news_data['Sentiment'] = news_data['Processed_Article'].apply(get_sentiment)\n",
        "\n",
        "# Define dimensions\n",
        "bert_dim = bert_model.config.hidden_size  # typically 768 for BERT models\n",
        "combined_dim = 1543  # Update this to the correct combined dimension\n",
        "\n",
        "# Initialize scalers\n",
        "scaler = StandardScaler()\n",
        "target_scalers = {ticker: StandardScaler() for ticker in companies_to_focus.keys()}\n",
        "\n",
        "# Simulate fitting scalers with initial data\n",
        "def fit_scalers():\n",
        "    combined_features_list = []\n",
        "    targets_list = []\n",
        "\n",
        "    for ticker in companies_to_focus.keys():\n",
        "        # Simulate fetching stock data\n",
        "        stock_data = yf.download(ticker, start='2021-01-01', end='2021-12-31')\n",
        "        stock_data.reset_index(inplace=True)\n",
        "\n",
        "        # Fetch moving averages\n",
        "        ma50 = stock_data['Close'].rolling(window=50).mean()\n",
        "        ma200 = stock_data['Close'].rolling(window=200).mean()\n",
        "\n",
        "        stock_data['MA50'] = ma50\n",
        "        stock_data['MA200'] = ma200\n",
        "\n",
        "        # Generate dummy combined features matching the expected combined dimension\n",
        "        num_samples = len(stock_data)\n",
        "        dummy_bert_features = np.zeros((num_samples, 768))  # Example BERT feature size\n",
        "        dummy_other_features = np.zeros((num_samples, combined_dim - 768))\n",
        "        combined_features = np.hstack([dummy_bert_features, dummy_other_features])\n",
        "\n",
        "        combined_features_list.append(combined_features)\n",
        "        targets_list.append(stock_data['Close'].values)\n",
        "\n",
        "    combined_features_array = np.concatenate(combined_features_list, axis=0)\n",
        "    targets_array = np.concatenate(targets_list, axis=0).reshape(-1, len(companies_to_focus))\n",
        "\n",
        "    scaler.fit(combined_features_array)\n",
        "    for i, ticker in enumerate(companies_to_focus.keys()):\n",
        "        target_scalers[ticker].fit(targets_array[:, i].reshape(-1, 1))\n",
        "\n",
        "fit_scalers()\n",
        "\n",
        "# Streamlit App Layout\n",
        "st.title(\"Stock Price Prediction App\")\n",
        "\n",
        "# Sidebar Description\n",
        "st.sidebar.title(\"About the App\")\n",
        "st.sidebar.markdown(\"\"\"\n",
        "This application predicts the stock prices of major companies using news headlines and sentiment analysis.\n",
        "We utilize BERT embeddings, technical indicators, and fundamental data for robust predictions.\n",
        "\"\"\")\n",
        "\n",
        "st.sidebar.title(\"Model Description\")\n",
        "st.sidebar.markdown(\"\"\"\n",
        "Our model leverages a transformer-based architecture with BERT embeddings to capture the semantic meaning of news articles.\n",
        "We incorporate technical indicators, such as moving averages, and fundamental data to improve the prediction accuracy.\n",
        "\"\"\")\n",
        "\n",
        "# Fetch data\n",
        "today = datetime.today().strftime('%Y-%m-%d')\n",
        "start_date = (datetime.today() - timedelta(days=365)).strftime('%Y-%m-%d')\n",
        "end_date = today\n",
        "\n",
        "# Get today's news headlines\n",
        "todays_news = news_data[news_data['Date'] == today]\n",
        "\n",
        "# Define dimensions\n",
        "bert_dim = bert_model.config.hidden_size  # typically 768 for BERT models\n",
        "combined_dim = 1543  # Update this to the correct combined dimension\n",
        "\n",
        "# Get stock data and predictions\n",
        "stock_data_dict = {}\n",
        "fundamental_data_dict = {}\n",
        "for ticker in companies_to_focus:\n",
        "    stock_data = yf.download(ticker, start=start_date, end=end_date)\n",
        "\n",
        "    # Ensure the Date column is present\n",
        "    stock_data.reset_index(inplace=True)\n",
        "\n",
        "    # Fetch moving averages\n",
        "    ma50 = stock_data['Close'].rolling(window=50).mean()\n",
        "    ma200 = stock_data['Close'].rolling(window=200).mean()\n",
        "\n",
        "    stock_data['MA50'] = ma50\n",
        "    stock_data['MA200'] = ma200\n",
        "\n",
        "    stock_data_dict[ticker] = stock_data\n",
        "    fundamental_data_dict[ticker] = fetch_fundamental_data(ticker)\n",
        "\n",
        "# Call predict_prices once\n",
        "news_headlines = todays_news['Processed_Article'].tolist()\n",
        "predictions = predict_prices(news_headlines, look_back, bert_dim, combined_dim, scaler, target_scalers)\n",
        "predictions_dict = {ticker: predictions[ticker] for ticker in companies_to_focus}\n",
        "\n",
        "# Display predicted prices for tomorrow\n",
        "st.subheader(\"Predicted Prices for Tomorrow\")\n",
        "for ticker, company in companies_to_focus.items():\n",
        "    today_price = stock_data_dict[ticker]['Close'].values[-1]\n",
        "    predicted_price = predictions_dict[ticker][0][0]  # Correct indexing to match prediction structure\n",
        "    arrow = \"⬆️\" if predicted_price > today_price else \"⬇️\"\n",
        "    color = \"green\" if predicted_price > today_price else \"red\"\n",
        "    st.markdown(f\"**{company} ({ticker}):** {predicted_price:.2f} {arrow}\", unsafe_allow_html=True)\n",
        "\n",
        "# Display news headlines with sentiment in a table\n",
        "st.subheader(\"Latest News\")\n",
        "news_table = todays_news[['News Article', 'Sentiment']].copy()\n",
        "news_table['Sentiment'] = news_table['Sentiment'].apply(lambda x: f\"<span style='color:{'green' if x > 0 else 'red'}'>{x:.2f}</span>\")\n",
        "st.write(news_table.to_html(escape=False, index=False), unsafe_allow_html=True)\n",
        "\n",
        "# Display stock price charts with actual, predicted prices, and technical indicators\n",
        "for ticker, company in companies_to_focus.items():\n",
        "    stock_data = stock_data_dict[ticker]\n",
        "    fig = go.Figure()\n",
        "\n",
        "    # Add actual stock price trace\n",
        "    fig.add_trace(go.Scatter(x=stock_data['Date'], y=stock_data['Close'], mode='lines', name='Actual Close'))\n",
        "\n",
        "    # Add predicted price trace\n",
        "    predicted_price = predictions_dict[ticker][0][0]\n",
        "    predicted_date = stock_data['Date'].iloc[-1] + timedelta(days=1)\n",
        "    fig.add_trace(go.Scatter(x=[predicted_date], y=[predicted_price], mode='markers', name='Predicted Close', marker=dict(color='red', size=10)))\n",
        "\n",
        "    # Add moving average traces\n",
        "    fig.add_trace(go.Scatter(x=stock_data['Date'], y=stock_data['MA50'], mode='lines', name='MA50'))\n",
        "    fig.add_trace(go.Scatter(x=stock_data['Date'], y=stock_data['MA200'], mode='lines', name='MA200'))\n",
        "\n",
        "    # Customize the layout\n",
        "    fig.update_layout(\n",
        "        title=f'{company} ({ticker}) Stock Prices',\n",
        "        xaxis_title='Date',\n",
        "        yaxis_title='Price',\n",
        "        showlegend=True\n",
        "    )\n",
        "\n",
        "    # Display the chart\n",
        "    st.plotly_chart(fig)\n",
        "\n",
        "    # Display fundamental data\n",
        "    st.subheader(f\"{company} ({ticker}) Fundamentals\")\n",
        "    fundamentals = fundamental_data_dict[ticker]\n",
        "    st.markdown(f\"\"\"\n",
        "    - **PE Ratio**: {fundamentals['PE_Ratio']}\n",
        "    - **EPS**: {fundamentals['EPS']}\n",
        "    - **Revenue**: {fundamentals['Revenue']}\n",
        "    - **Market Cap**: {fundamentals['Market_Cap']}\n",
        "    \"\"\")\n",
        "\n",
        "# Manual prediction input\n",
        "st.subheader(\"Manual Prediction Input\")\n",
        "manual_news_headlines = st.text_area(\"Enter News Headlines\", \"\").split('\\n')\n",
        "\n",
        "if st.button(\"Predict Manually\"):\n",
        "    if manual_news_headlines:\n",
        "        manual_predictions = predict_prices(manual_news_headlines, look_back_window, bert_dim, combined_dim, scaler, target_scalers)\n",
        "        for ticker, company in companies_to_focus.items():\n",
        "            manual_prediction = manual_predictions[ticker][0][0]\n",
        "            st.write(f\"Predicted price for {company} ({ticker}): {manual_prediction:.2f}\")\n",
        "\n",
        "# \"See More\" Section\n",
        "st.subheader(\"See More\")\n",
        "st.markdown(\"\"\"\n",
        "We also trained a model that uses Topic Modelling, TF-IDF, and Named Entity Recognition (NER) as features.\n",
        "For more details, check out our [GitHub Repository](https://github.com/your-repo-link).\n",
        "\"\"\")\n",
        "\n",
        "# End of the Streamlit app"
      ],
      "metadata": {
        "id": "xZjwPj_xuP-Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install streamlit pyngrok\n",
        "\n",
        "from pyngrok import ngrok\n",
        "\n",
        "# Initialize ngrok\n",
        "ngrok.set_auth_token('2h6sfydzZ90UINBPKt8DX3rmu1h_6jzyuJACPKAhjFUh64RAx')  # Get your auth token from ngrok\n",
        "\n",
        "# Kill any previous tunnels if open\n",
        "ngrok.kill()\n",
        "\n",
        "# Start a new ngrok tunnel\n",
        "public_url = ngrok.connect(8501)\n",
        "\n",
        "# Display the public URL\n",
        "print(f'Streamlit App is available at: {public_url}')\n",
        "\n",
        "# Run the Streamlit app\n",
        "!streamlit run app.py &>/dev/null&"
      ],
      "metadata": {
        "id": "nvXsJFWLyl_P"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}